---
title: Fairness Definition Literature
sidebar_position: 3
slug: /fairness-definition-literature
---

# Fairness Definition Literature

A catalog of common group and individual fairness definitions with concise explanations.

> Need help choosing a metric? Go to the [Fairness Definition Selection Tool](/docs/fairness-definition-selection).

## **1 Group Fairness Metrics**

### 1.1 Demographic Parity (Statistical Parity)

**Goal**

Ensure that all demographic groups receive positive predictions at the same rate.

**Definition**

$$
P(\hat{Y} = 1 \mid A = a) = P(\hat{Y} = 1 \mid A = b)
$$

- $\hat{Y}$: predicted outcome
- $A$: protected attribute

> ***Example:*** If a loan approval model approves 40% of applicants from Group A, it should also approve 40% from Group B, regardless of who actually repays.

----

### 1.2 Equal Opportunity

**Goal**

Ensure equal true positive rates across demographic groups.

**Definition**

$$
P(\hat{Y} = 1 \mid Y = 1, A = a) = P(\hat{Y} = 1 \mid Y = 1, A = b)
$$

- $Y$: true outcome
- $\hat{Y}$: predicted outcome
- $A$: protected attribute

> ***Example:*** Among applicants who would actually repay a loan, the model should approve the same proportion from each demographic group.

----

### 1.3 Equalized Odds

**Goal**

Ensure equal true positive rates **and** equal false positive rates across groups.

**Definition**

$$
P(\hat{Y} = 1 \mid Y = y, A = a) = P(\hat{Y} = 1 \mid Y = y, A = b)
$$

- for $y$ in {0, 1}: applies to both positive and negative true outcomes
- $Y$: true outcome
- $\hat{Y}$: predicted outcome
- $A$: protected attribute

> ***Example:*** The model should be equally likely to correctly approve good borrowers and equally likely to mistakenly approve bad borrowers across all groups.

----

### 1.4 Predictive Parity

**Goal**

Ensure that positive predictions are equally accurate across groups.

**Definition**

$$
P(Y = 1 \mid \hat{Y} = 1, A = a) = P(Y = 1 \mid \hat{Y} = 1, A = b)
$$

- $Y$: true outcome
- $\hat{Y}$: predicted outcome
- $A$: protected attribute

> ***Example:*** Among applicants who are approved, the fraction who actually repay the loan should be the same for all demographic groups.

## **2 Individual Fairness Metrics**

### 2.1 Similarity-Based Fairness

**Goal**

Ensure that similar individuals receive similar predictions.

**Equation**

$$
d_Y(\hat{Y}(x_i), \hat{Y}(x_j)) \le L \cdot d_X(x_i, x_j)
$$

Where:

- $d_X$ is a distance metric in the input (feature) space
- $d_Y$ is a distance metric in the output (prediction) space
- $L$ is a Lipschitz constant controlling sensitivity
- $\hat{Y}$ is the predicted value

> ***Example:*** If two job applicants have nearly identical qualifications, the difference in their predicted hiring scores should be small.

----

### 2.2 Fairness Through Awareness

**Goal**

Achieve fairness by explicitly defining a task-specific notion of similarity.

**Equation (conceptual form)**

$$
\hat{Y}(x_i) \approx \hat{Y}(x_j)
$$

When the input distance $d_X(x_i, x_j)$ is small, predictions should remain close.

Where:

- $\hat{Y}$ is the predicted value
- $d_X$ is a similarity metric designed for the task
- Protected attributes are excluded or neutralized in $d_X$

> ***Example:*** In college admissions, two students with similar academic records should receive similar admission scores, even if they differ in race or gender.

----

### 2.3 Counterfactual Fairness

**Goal**

Ensure that predictions are independent of protected attributes under counterfactual changes.

**Equation**

$$
\hat{Y}_{A \leftarrow a}(U) = \hat{Y}_{A \leftarrow b}(U)
$$

Where:

- $A$ is a protected attribute
- $U$ represents background variables
- The prediction is compared across counterfactual worlds where only $A$ changes

> ***Example:*** A credit score is counterfactually fair if it would remain the same in a hypothetical world where only the individualâ€™s race or gender were different.
