<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-bias-type-literature" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Bias Type Literature | Fairness Audit Playbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://herkata.github.io/fairness-audit/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://herkata.github.io/fairness-audit/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://herkata.github.io/fairness-audit/docs/bias-type-literature"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Bias Type Literature | Fairness Audit Playbook"><meta data-rh="true" name="description" content="Reference taxonomy for common bias sources with definitions, indicators, and examples."><meta data-rh="true" property="og:description" content="Reference taxonomy for common bias sources with definitions, indicators, and examples."><link data-rh="true" rel="icon" href="/fairness-audit/img/katkatu-merleg.svg"><link data-rh="true" rel="canonical" href="https://herkata.github.io/fairness-audit/docs/bias-type-literature"><link data-rh="true" rel="alternate" href="https://herkata.github.io/fairness-audit/docs/bias-type-literature" hreflang="en"><link data-rh="true" rel="alternate" href="https://herkata.github.io/fairness-audit/docs/bias-type-literature" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Bias Type Literature","item":"https://herkata.github.io/fairness-audit/docs/bias-type-literature"}]}</script><link rel="alternate" type="application/rss+xml" href="/fairness-audit/blog/rss.xml" title="Fairness Audit Playbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/fairness-audit/blog/atom.xml" title="Fairness Audit Playbook Atom Feed"><link rel="stylesheet" href="/fairness-audit/assets/css/styles.69bbdfb0.css">
<script src="/fairness-audit/assets/js/runtime~main.6d003bb2.js" defer="defer"></script>
<script src="/fairness-audit/assets/js/main.0fb0aa49.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/fairness-audit/img/katkatu-merleg.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/fairness-audit/"><div class="navbar__logo"><img src="/fairness-audit/img/katkatu-merleg.svg" alt="Kétkarú mérleg" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/fairness-audit/img/katkatu-merleg.svg" alt="Kétkarú mérleg" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Fairness Audit Playbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/fairness-audit/docs/fairness-definition-literature">Literature</a><a class="navbar__item navbar__link" href="/fairness-audit/docs/implementation-guide">Tools</a><a class="navbar__item navbar__link" href="/fairness-audit/docs/sources">Sources and Plans</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Herkata/fairness-audit" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fairness-audit/docs/fairness-definition-literature"><span title="Fairness Definition Literature" class="linkLabel_WmDU">Fairness Definition Literature</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/fairness-audit/docs/bias-type-literature"><span title="Bias Type Literature" class="linkLabel_WmDU">Bias Type Literature</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fairness-audit/docs/metric-definition-literature"><span title="Metric Definition Literature" class="linkLabel_WmDU">Metric Definition Literature</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/fairness-audit/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Bias Type Literature</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Bias Type Literature</h1></header>
<p>Reference taxonomy for common bias sources with definitions, indicators, and examples.</p>
<blockquote>
<p>Need to investigate and log findings? Use the <a class="" href="/fairness-audit/docs/bias-source-identification">Bias Source Identification Tool</a>.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-data-based-bias"><strong>1 Data-based Bias</strong><a href="#1-data-based-bias" class="hash-link" aria-label="Direct link to 1-data-based-bias" title="Direct link to 1-data-based-bias" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="11-historical-bias">1.1 Historical Bias<a href="#11-historical-bias" class="hash-link" aria-label="Direct link to 1.1 Historical Bias" title="Direct link to 1.1 Historical Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias resulting from pre-existing social inequities, regardless of sampling or feature selection.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Target variables reflecting historical discrimination.</li>
<li class="">Problematic correlations that mirror societal inequities.</li>
<li class="">Patterns that align with known historical discrimination.</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A hiring algorithm trained on historical hiring decisions may perpetuate patterns of gender discrimination in technical roles.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="12-representation-bias">1.2 Representation Bias<a href="#12-representation-bias" class="hash-link" aria-label="Direct link to 1.2 Representation Bias" title="Direct link to 1.2 Representation Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from how populations are sampled and measured in training data.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Demographic imbalances compared to target population</li>
<li class="">Quality disparities across demographic groups</li>
<li class="">Systematic measurement differences</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A medical diagnostic system trained primarily on data from young adult males may perform poorly for elderly female patients.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="13-measurement-bias">1.3 Measurement Bias<a href="#13-measurement-bias" class="hash-link" aria-label="Direct link to 1.3 Measurement Bias" title="Direct link to 1.3 Measurement Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from how attributes are measured, proxied, or operationalized.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Different measurement approaches across groups</li>
<li class="">Proxy variables with varying accuracy across populations</li>
<li class="">Inconsistent label quality across demographics</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> Using standardized test scores as a proxy for aptitude may disadvantage groups with less access to test preparation resources.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-methodology-bias"><strong>2 Methodology Bias</strong><a href="#2-methodology-bias" class="hash-link" aria-label="Direct link to 2-methodology-bias" title="Direct link to 2-methodology-bias" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="21-aggregation-bias">2.1 Aggregation Bias<a href="#21-aggregation-bias" class="hash-link" aria-label="Direct link to 2.1 Aggregation Bias" title="Direct link to 2.1 Aggregation Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from combining distinct populations that may have different relationships between features and outcomes.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">One-size-fits-all models for heterogeneous populations</li>
<li class="">Features with different predictive relationships across groups</li>
<li class="">Unexplained performance disparities across subgroups</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A credit scoring model might not account for different cultural approaches to credit usage, creating disparities across ethnic groups.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="22-learning-bias">2.2 Learning Bias<a href="#22-learning-bias" class="hash-link" aria-label="Direct link to 2.2 Learning Bias" title="Direct link to 2.2 Learning Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from modeling choices that amplify or create disparities.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Algorithms that overfit majority patterns</li>
<li class="">Regularization approaches that penalize minority patterns</li>
<li class="">Optimization objectives misaligned with fairness goals</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A complex model might learn spurious correlations between protected attributes and outcomes that don&#x27;t represent causal relationships.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="23-evaluation-bias">2.3 Evaluation Bias<a href="#23-evaluation-bias" class="hash-link" aria-label="Direct link to 2.3 Evaluation Bias" title="Direct link to 2.3 Evaluation Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from testing procedures that don&#x27;t represent real-world performance or fairness.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Test datasets with different characteristics than deployment contexts</li>
<li class="">Metrics that don&#x27;t capture relevant fairness dimensions</li>
<li class="">Insufficient disaggregation of performance across groups</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> Evaluating a facial recognition system on a test set that doesn&#x27;t include diverse skin tones will mask potential performance disparities in deployment.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-circumstantial-bias"><strong>3 Circumstantial Bias</strong><a href="#3-circumstantial-bias" class="hash-link" aria-label="Direct link to 3-circumstantial-bias" title="Direct link to 3-circumstantial-bias" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-deployment-bias">3.1 Deployment Bias<a href="#31-deployment-bias" class="hash-link" aria-label="Direct link to 3.1 Deployment Bias" title="Direct link to 3.1 Deployment Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from how systems are implemented and used in practice.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Context shifts between training and deployment</li>
<li class="">User interactions that reinforce biases</li>
<li class="">Feedback loops that amplify initial disparities</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A recommendation system might create filter bubbles that limit exposure diversity based on initial demographic patterns.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-infrastructure-and-resource-disparity-bias">3.2 Infrastructure and Resource Disparity Bias<a href="#32-infrastructure-and-resource-disparity-bias" class="hash-link" aria-label="Direct link to 3.2 Infrastructure and Resource Disparity Bias" title="Direct link to 3.2 Infrastructure and Resource Disparity Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from unequal access to technological infrastructure, digital literacy, or resources required to benefit from AI systems.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Unequal access to required hardware or internet connectivity</li>
<li class="">Differences in digital literacy across populations</li>
<li class="">System performance depending on device quality or data availability</li>
<li class="">Disparities in adoption rates across socioeconomic groups</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> An AI-powered loan application system may be equally fair in design, but individuals with limited internet access or lower digital literacy may struggle to complete applications accurately, leading to unequal access to credit.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="33-organizational-implementation-bias">3.3 Organizational Implementation Bias<a href="#33-organizational-implementation-bias" class="hash-link" aria-label="Direct link to 3.3 Organizational Implementation Bias" title="Direct link to 3.3 Organizational Implementation Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising from how an AI system is implemented, governed, and integrated within organizational processes and incentive structures.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Lack of accountability or fairness monitoring after deployment</li>
<li class="">Incentive systems prioritizing efficiency over equity</li>
<li class="">Inconsistent override policies across decision-makers</li>
<li class="">Absence of diverse perspectives in governance processes</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A loan approval model designed with fairness safeguards may still produce discriminatory outcomes if organizational policies encourage aggressive cost-cutting or allow discretionary overrides that disproportionately disadvantage certain groups.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-bias-feeding-processes"><strong>4 Bias-feeding Processes</strong><a href="#4-bias-feeding-processes" class="hash-link" aria-label="Direct link to 4-bias-feeding-processes" title="Direct link to 4-bias-feeding-processes" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="41-direct-feedback-loop">4.1 Direct Feedback Loop<a href="#41-direct-feedback-loop" class="hash-link" aria-label="Direct link to 4.1 Direct Feedback Loop" title="Direct link to 4.1 Direct Feedback Loop" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising when a system’s outputs directly shape future inputs within the same decision pipeline, reinforcing prior predictions or outcomes.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Repeated reinforcement of earlier decisions</li>
<li class="">Declining diversity in outputs over time</li>
<li class="">Increasing confidence in patterns that originated from initial model behavior</li>
<li class="">Self-reinforcing approval or rejection cycles</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A loan approval system that approves applicants from a specific demographic more frequently will collect more repayment data from that group. Over time, the model may become increasingly confident about that group while remaining uncertain about underrepresented groups, reinforcing disparities.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="42-indirect-feedback-loop">4.2 Indirect Feedback Loop<a href="#42-indirect-feedback-loop" class="hash-link" aria-label="Direct link to 4.2 Indirect Feedback Loop" title="Direct link to 4.2 Indirect Feedback Loop" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising when a system’s outputs influence future inputs through intermediate processes, institutions, or external behaviors.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Downstream behavioral changes affecting future data</li>
<li class="">Institutional processes shaped by prior model outputs</li>
<li class="">Long-term shifts in applicant pools</li>
<li class="">Indirect amplification of initial disparities</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> If certain applicants are consistently denied loans, they may stop applying altogether or seek alternative financial services. Over time, the applicant pool shifts, and the model is trained on increasingly selective data, reinforcing exclusion patterns.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="43-user-driven-feedback-bias">4.3 User-Driven Feedback Bias<a href="#43-user-driven-feedback-bias" class="hash-link" aria-label="Direct link to 4.3 User-Driven Feedback Bias" title="Direct link to 4.3 User-Driven Feedback Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias emerging from users adapting their behavior strategically or unintentionally in response to an AI system’s decision logic.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Applicants modifying inputs to optimize approval chances</li>
<li class="">Use of learned “trigger” features that influence predictions</li>
<li class="">Distribution shifts in input data over time</li>
<li class="">Increased homogeneity in application patterns</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> Loan applicants may learn that certain employment types or reported income structures increase approval likelihood and adjust their reporting behavior accordingly, distorting input distributions and potentially disadvantaging less informed applicants.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="44-system-driven-feedback-bias">4.4 System-Driven Feedback Bias<a href="#44-system-driven-feedback-bias" class="hash-link" aria-label="Direct link to 4.4 System-Driven Feedback Bias" title="Direct link to 4.4 System-Driven Feedback Bias" translate="no">​</a></h3>
<ul>
<li class=""><strong>Definition:</strong> Bias arising when an AI system autonomously updates or adapts based on new data, potentially amplifying existing disparities without explicit user intervention.</li>
<li class=""><strong>Indicators:</strong>
<ul>
<li class="">Performance drift across demographic groups over time</li>
<li class="">Gradual changes in decision thresholds through automated retraining</li>
<li class="">Increasing disparities following continuous learning updates</li>
<li class="">Lack of periodic fairness re-evaluation after model updates</li>
</ul>
</li>
</ul>
<blockquote>
<p><em><strong>Example:</strong></em> A continuously retrained loan risk model may increasingly weight features correlated with protected attributes as new data accumulates, leading to progressively larger disparities if fairness constraints are not re-applied during retraining.</p>
</blockquote></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Herkata/fairness-audit/tree/main/docs/05-bias-type-literature.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/fairness-audit/docs/fairness-definition-literature"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Fairness Definition Literature</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/fairness-audit/docs/metric-definition-literature"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Metric Definition Literature</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-data-based-bias" class="table-of-contents__link toc-highlight"><strong>1 Data-based Bias</strong></a><ul><li><a href="#11-historical-bias" class="table-of-contents__link toc-highlight">1.1 Historical Bias</a></li><li><a href="#12-representation-bias" class="table-of-contents__link toc-highlight">1.2 Representation Bias</a></li><li><a href="#13-measurement-bias" class="table-of-contents__link toc-highlight">1.3 Measurement Bias</a></li></ul></li><li><a href="#2-methodology-bias" class="table-of-contents__link toc-highlight"><strong>2 Methodology Bias</strong></a><ul><li><a href="#21-aggregation-bias" class="table-of-contents__link toc-highlight">2.1 Aggregation Bias</a></li><li><a href="#22-learning-bias" class="table-of-contents__link toc-highlight">2.2 Learning Bias</a></li><li><a href="#23-evaluation-bias" class="table-of-contents__link toc-highlight">2.3 Evaluation Bias</a></li></ul></li><li><a href="#3-circumstantial-bias" class="table-of-contents__link toc-highlight"><strong>3 Circumstantial Bias</strong></a><ul><li><a href="#31-deployment-bias" class="table-of-contents__link toc-highlight">3.1 Deployment Bias</a></li><li><a href="#32-infrastructure-and-resource-disparity-bias" class="table-of-contents__link toc-highlight">3.2 Infrastructure and Resource Disparity Bias</a></li><li><a href="#33-organizational-implementation-bias" class="table-of-contents__link toc-highlight">3.3 Organizational Implementation Bias</a></li></ul></li><li><a href="#4-bias-feeding-processes" class="table-of-contents__link toc-highlight"><strong>4 Bias-feeding Processes</strong></a><ul><li><a href="#41-direct-feedback-loop" class="table-of-contents__link toc-highlight">4.1 Direct Feedback Loop</a></li><li><a href="#42-indirect-feedback-loop" class="table-of-contents__link toc-highlight">4.2 Indirect Feedback Loop</a></li><li><a href="#43-user-driven-feedback-bias" class="table-of-contents__link toc-highlight">4.3 User-Driven Feedback Bias</a></li><li><a href="#44-system-driven-feedback-bias" class="table-of-contents__link toc-highlight">4.4 System-Driven Feedback Bias</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/fairness-audit/docs/implementation-guide">Implementation Guide</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Herkata/fairness-audit" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Fairness Audit Playbook. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>